根据西瓜书和统计学习方法总结。

## 基本概念

* 数据集
* 假设空间
* 归纳偏好
* 生成模型和判别模型
* 正则化和交叉验证
* 泛化能力
* 分类问题
* 标注问题
* 归类问题

## 模型评估和选择

* 过拟合欠拟合
* 评估方法
    * 留出法
    * 交叉验证法
    * 自助法
    * 调参与最终模型
* 性能度量
    * 错误率与精度，分母是样本集
    * 精确度和召回率，分母是正判别数和正样本数
    * ROC和AUC，分母是正样本数和负样本数，正样本召回率和负样本召回率
* 比较检验  【待深究】
    * 假设检验， 做了个高斯模型检验
    * 交叉验证t检验
    * McNemar检验
    * Friedman和Nemenyi后续检验
* 偏差和方差——性能检验， 偏差、方差、噪声

## 线性模型

* 线性回归
* 对数几率回归 logistic regression 最大似然证明，模糊
* 线性判别分析，将多点映射到一条线上，保证高内聚低耦合 LDA
* 多分类学习   OvsO, OvsR, MvsM
* 类别不平衡问题  欠采样多的，过采样少的，再缩放
* 稀疏表示

## 决策树

* 基本流程 递归生成决策树
* 划分选择
    * 信息熵增益
    * 信息熵增益率
    * 基尼系数
* 剪枝处理 解决过拟合
    * 预剪枝
    * 后剪枝
* 连续与缺失值
    * 连续值做分割
    * 缺失值直接在熵计算的分子分母中同时去除
* 多变量决策树 解决决策树变量只能在二维空间中水平或竖直划分的问题，构造斜分甚至是曲线分类比较困难。

## 神经网络

* 神经元模型 使用线性模型，将多个变量输入+阈值控制输出激活和抑制 模拟人类神经元的模型；
* 感知机和多层神经网络 感知机由两层神经网络构成，多层神经网络指的是中间有多层隐层的神经网络，输入影响隐层，然后隐层影响输出层，理论上层数越多可以模拟任何非线性函数；也叫前反馈神经网络
* 误逆差传播算法 (BP神经网络)  最小化输出方差 通过求导梯度下降更新输入权重； 可分为标准BP算法(每次更新一个样例数据，但是多个样例中间可能会抵消)和累积BP算法(计算所有样本方差后，再对w和阈值进行更新 但是下降到一定程度后就停住了，效果没有快速迭代的标准BP好)；
* 局部最小和全局最小，matlab的logo图；通过引入随机值跳出局部最小；
* 其它常见的神经网络
    * RBF神经网络(隐函数激活使用径向基函数，如高斯函数、等非线性函数)、
    * ART网络(竞争学习，神经元激活是互斥的，可调节阈值，设置竞争成功后抑制范围)、
    * SOM网络(竞争神经网络，高维映射低维并保留空间关系，取距离最小的胜出竞争)、
    * 级联相关网络
    * Elman网络
    * Boltzmann机
* 深度学习 多层学习，但是由于多层之间的反馈太长，导致训练不收敛，会进行单层训练(pretrain)收敛后再连到一起进行微调；

## 支持向量机

* 间隔与支持向量机， 推导为最近的几个点确定了向量，保证集合最近的点离超平面的距离最大
* 对偶问题
* 核函数   对于不是线性可分的样本，选择核函数或者增加维度让其线性可分，核函数可以做线性叠加
* 软间隔与正则化
* 支持向量回归
* 核方法

## 贝叶斯分类器

* 贝叶斯决策论  贝叶斯公式，假设参数和结果是独立同分布的，条件概率 可以通过 最小化分类错误率完成参数设定；
* 极大似然估计  通过现有的数据，计算极大似然（即给定什么参数时，当前数据出现的概率最大），使用对数似然避免连乘操作下溢；
* 朴素贝叶斯分类器 假设每个参数都是独立分布，计算最大似然，在出现概率值为0时，采用分子+1(未知)，分母+N(所有取值)的方式避免；
* 半朴素贝叶斯 假设参数间有关系，但只有一个所有参数的父级参数或每个参数都只有一个父级参数，构造： 计算任意两个参数间的条件互信息，构建完全图，给边增加权重，选择最大带权生成树；
* 贝叶斯网 评分搜索贝叶斯网，一般通过信息论编码最短准则构建网络，构建网路后一样计算最大似然确定参数；
* EM算法 EM算法是解决未观测到的变量常用的算法，分为E step和M step，E step通过已知参数计算当前的未知参数，M step通过最大期望似然计算新的似然值；通过迭代收敛直到 E 不变，则确定训练参数，k-means聚类是经典的EM算法；

## 集成学习

* 个体与集成 将个体的学习算法集成为集合算法，一起决策；多样性、差异性；
* Bossting 著名算法 AdaBoost 是加性模型，对每个基模型累加，指数化最小损失函数；
* Bagging与随机森林
    * Bagging 随机重复采样，加入样本扰动，保证有 1 - 1/e概率的样本出现在使用的样本集中；
    * 随机森林 在Bagging的基础上，加上属性扰动(属性随机选择k个，再进行最有属性选择)；
* 结合策略 几种集成学习的结合策略
    * 平均法，加权平均；
    * 投票法，加权投票法；
    * 学习法，用初级的训练结果作为输入数据，训练次级结合模型；
* 多样性
    * 误差-分歧分解 个体学习的精确度越高，多样性越大则集成越好；
    * 多样性度量 不合度量、相关系数、Q-统计量、k-统计量；
    * 多样性增强 数据样本扰动、输入属性扰动、输出表示扰动、算法参数扰动；

## 聚类

无监督学习

* 聚类任务 性能度量、距离计算；
* 性能度量 通过计算参考聚类与模型聚类的各个指数 Jaccard系数、FM指数、Rand指数、BD指数、DUNN指数等；
* 距离计算 对称性、直递性(三角形两边之和大于第三边)；
* 原型聚类 先通过原型刻画聚类结构，再通过迭代确定参数；
    * k-means聚类 计算类中心，然后计算距离完成分类，然后再计算中心； EM算法；
    * 学习向量量化 与k-means类似，不过是有监督学习，有已标注的数据进行调整；
    * 高斯混合聚类 使用多个高斯分布作为聚类模型，并用贝叶斯的后验概率作为距离；
    * 密度聚类
    * 层次聚类

## 降维和度量学习

* k邻近学习 监督学习，需要大样本密度，在指定距离内找到邻近样本；
* 低维嵌入 增加样本密度，减少距离损失；
* 主成分分析 样本点离超平面最近 + 样本点在低维中离散 这两个目标一致（高维中的椭圆肯定是按照长轴降维，此时同事满足这两个条件）；
* 核化线性降维 高维空间降维过程中非线性，则选择非线性核，解决S型降维问题；
* 等度量映射 同样考虑S型降维，在局部是线性的，则进行据不限行累加(使用dijkstra算法累加)；
* 局部线性嵌入 保留局部线性关系 【不太明白】
* 度量学习 将降维作为学习目的，将分类性能作为目标，确定参数；

## 特征选择和稀疏学习

* 子集搜索与评价
    * 通过信息熵增益来筛选，跟决策树类似
    * 过滤式选择，通过Relief计算输入x与输出y之间的相关性 (近似样本计算最邻近-|x1-x2|距离，反例样本计算最邻近|x1-x2|)，设置阈值选取属性
    * 包裹式选择，通过随机选择特征+算法学习迭代完成特征选择，精确但成本高
    * 嵌入式选择，通过L1、L2正则化来达到稀疏特征的目的
    * 稀疏表示与字典学习，将特征在高维稀疏展开，使其变得线性可分，同时稀疏矩阵有存储优势，比如分析文本内容
    * 压缩感知，通过低频采样来完成复现高频序列，虽然矩阵不正定无解，但是在矩阵稀疏时，可求解【待理解】

## 计算学习理论

* TODO

## 半监督学习

* 未标记样本；
* 生成式方法  未标记样本对生成模型的估计是有用的，半监督可以理解为开始的监督过程是变量初始化，后续的学习过程是模型参数优化；例子： 混合高斯模型的半监督迭代；
* 半监督SVM  未标记样本不断更新分割面；
* 图半监督学习  类似的，在图中初始化几个分类，然后通过图节点间相关性作为距离进行分类着色；
* 基于分歧的方法  多分类器时，产生分歧需要解决，一般用置信度来采纳；
* 半监督聚类  初始化几个节点，完成k-means聚类；

## 概率图模型 【深入理解】

* 隐马尔可夫模型   在语音识别中有应用，状态变换+观测现象建模；下一个状态只依赖于上一个状态；
* 马尔可夫随机场  由链变成五向图；
* 条件随机场 隐马尔可夫链和马尔可夫随机场是对生成模型的建模，条件随机场是对条件模型的建模；
* 学习和推断
    * 变量消去
    * 信念传播
* 近似推断
    * MCMC采样
    * 变分推断
* 话题模型

## 规则学习 【深入理解】

* 基本概念  规则学习是通过逻辑符号组成的规则来描述规律的一种表示方法；
* 序贯覆盖  慢慢用规则覆盖样本；
* 剪枝优化  规则生成是贪心搜索过程，剪枝优化可以防止过拟合；
* 一阶规则学习
* 归纳逻辑程序设计
    * 最小一般泛化
    * 逆归结

## 强化学习

* 任务与奖赏 行动和奖赏
* K-摇臂赌博机
    * 探索和利用 探索指的是随机选取一个杠，然后计算期望，利用指的是选取期望最大的；
    * E贪心 以e的概率进行探索，以1-e的概率进行利用；e可以随时间的增长而减小；
    * Softmax 通过设置阈值达到控制 探索、利用 的概率

